<!DOCTYPE html>
<html>
	<head>
		<meta charset="UTF-8" />
		<title>Programmation et Projet encadré</title>
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css" />
		<meta name="viewport" content="width=device-width, initial-scale=1">
	</head>
	<body class="has-navbar-fixed-top">
		<!--NAVBAR-->
		<nav class="navbar is-light is-fixed-top">
			<div class="navbar-menu">
				<div class="navbar-start">
					<div class="navbar-item has-dropdown is-hoverable">
						<a class="navbar-item" href="index.html#introduction">Introduction</a>
					</div>
					<div class="navbar-item has-dropdown is-hoverable">
						<a class="navbar-item" href="index.html#analyse">Analyse</a>
					</div>
					<div class="navbar-item has-dropdown is-hoverable">
						<a class="navbar-item" href="scripts.html">Scripts</a>
					</div>
					<div class="navbar-item has-dropdown is-hoverable">
						<a class="navbar-item">Tableaux</a>
						<div class="navbar-dropdown">
							<a class="navbar-item" href="tableaux/tableau_fr.html">
							  Français
							</a>
							<a class="navbar-item" href="tableaux/tableau_ru.html">
							  Russe
							</a>
							<a class="navbar-item" href="tableaux/tableau_zh.html">
							  Chinois
							</a>
						  </div>
					</div>
				</div>
				<div class="navbar-end">
					<div class="navbar-item has-dropdown is-hoverable">
						<a class="navbar-item" href="index.html#aPropos">À propos</a>
					</div>
					<div class="navbar-item">
						<a href="https://github.com/cbuontal/M1TAL_immigration"><img src="images/github_logo.png" alt="Github"></a>
					</div>
				</div>
			</div>
		</nav>
		<!--HEADER-->
		<header style="background-color: #06213e;">
			<img src="images/header.png" style="display: block; margin-left: auto; margin-right: auto; width: 50%;" alt="(IM|É)MIGRATION - Projet PPE">
		</header>
		<!--CONTENU-->
		<div class="has-text-justified", style="background-color: white;">
			<!--traitement des urls-->
			<h1 class="title has-text-centered" style="color: #a92927; font-size: 40px; letter-spacing: -0.02em; font-variant-caps: small-caps;">Traitement des URLs</h1>
			<div class="columns">
				<div class="column is-one-fifth"></div>
				<div class="column" style="border-left: 5px solid #a92927; overflow: auto; max-height: 1000px;">
					<p class="block">
						Ce programme nous permet de traiter toutes les urls d'un fichier afin de réaliser un tableau comportant les liens, des informations sur ceux-ci, les dumps des liens et les contextes et concordances. Il fait appel au script concordance.sh.
					</p>
					<pre>
						<code>
#!/usr/bin/env bash

#===============================================================================
# VOUS DEVEZ MODIFIER CE BLOC DE COMMENTAIRES.
# Ici, on décrit le comportement du programme.
# Indiquez, entre autres, comment on lance le programme et quels sont
# les paramètres.
# La forme est indicative, sentez-vous libres d'en changer !
# Notamment pour quelque chose de plus léger, il n'y a pas de norme en bash.
#
#
#	Utilisation : bash traitement_url_base.sh nom_fichier_URL nom_fichier_HTML
#
#
#===============================================================================

# !!!!!!
# ici on doit vérifier que nos deux paramètres existent, sinon on ferme!
# !!!!!!


if [ $# -ne 3 ]
then
	echo "Il faut trois paramètres."
	echo "Utilisation : bash traitement_url_base.sh &lt;langue&gt; &lt;nom_fichier_URL&gt; &lt;nom_fichier_HTML&gt;"
	exit
fi

lang=$1
fichier_urls=$2 # le fichier d'URL en entrée
fichier_tableau=$3 # le fichier HTML en sortie

basename=$(basename -s .txt $fichier_urls)

if [[ $lang == 'zh' ]]
then
	mot="移民"
elif [[ $lang == 'ru' ]]
then
		mot="(иммигр\w+|эмигр\w+)"
elif [[ $lang == 'fr' ]]
then
	mot="([Éé]migr\w+|[Ii]mmigr\w+)"
fi


# on utilise la commande :
# curl -I lien.html
# pour obtenir seulement l'entête de la réponse du serveur
# -L pour gérer les réponses 301 "moved permanently"
# -s pour le mode silencieux et ne pas polluer l'affichage du terminal

# la réponse se trouve dans la première ligne du résultat, deuxième élément
# on peut l'obtenir en extrayant la première ligne et en sélectionnant son 2è élément


# curl -I nom_url | grep HTTP | cut -d ' ' -f 2

echo 	"&lt;html&gt;
	&lt;head&gt;
		&lt;meta charset=\"utf-8\" /&gt;
		&lt;link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css\"&gt;
		&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt;
		&lt;title&gt;Tableau des URLS&lt;/title&gt;
	&lt;/head&gt;
	&lt;body class=\"has-navbar-fixed-top\"&gt;
		&lt;nav class=\"navbar is-light is-fixed-top\"&gt;&lt;div class=\"navbar-menu\"&gt;&lt;div class=\"navbar-start\"&gt;&lt;div class=\"navbar-item has-dropdown is-hoverable\"&gt;&lt;a class=\"navbar-item\" href=\"../index.html#introduction\"&gt;Introduction&lt;/a&gt;&lt;/div&gt;&lt;div class=\"navbar-item has-dropdown is-hoverable\"&gt;&lt;a class=\"navbar-item\" href=\"../index.html#analyse\"&gt;Analyse&lt;/a&gt;&lt;/div&gt;&lt;div class=\"navbar-item has-dropdown is-hoverable\"&gt;&lt;a class=\"navbar-item\" href=\"../scripts.html\"&gt;Scripts&lt;/a&gt;&lt;/div&gt;&lt;div class=\"navbar-item has-dropdown is-hoverable\"&gt;&lt;a class=\"navbar-item\"&gt;Tableaux&lt;/a&gt;&lt;div class=\"navbar-dropdown\"&gt;&lt;a class=\"navbar-item\" href=\"tableau_fr.html\"&gt;Français&lt;/a&gt;&lt;a class=\"navbar-item\" href=\"tableau_ru.html\"&gt;Russe&lt;/a&gt;&lt;a class=\"navbar-item\" href=\"tableau_zh.html\"&gt;Chinois&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=\"navbar-end\"&gt;&lt;div class=\"navbar-item has-dropdown is-hoverable\"&gt;&lt;a class=\"navbar-item\" href=\"../index.html#aPropos\"&gt;À propos&lt;/a&gt;&lt;/div&gt;&lt;div class=\"navbar-item\"&gt;&lt;a href=\"https://github.com/cbuontal/M1TAL_immigration\"&gt;&lt;img src=\"../images/github_logo.png\" alt=\"Github\"&gt;&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/nav&gt;
		&lt;h1 class=\"title\"&gt;Tableau des URLs $basename&lt;/h1&gt;
		&lt;table class=\"table is-bordered\"&gt;
			&lt;thead&gt;&lt;tr&gt;&lt;th&gt;ligne&lt;/th&gt;&lt;th&gt;code HTTP&lt;/th&gt;&lt;th&gt;URL&lt;/th&gt;&lt;th&gt;encodage&lt;/th&gt;&lt;th&gt;html&lt;/th&gt;&lt;th&gt;dump&lt;/th&gt;&lt;th&gt;occurrences&lt;/th&gt;&lt;th&gt;contextes&lt;/th&gt;&lt;th&gt;concordances&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;" &gt; "tableaux/$fichier_tableau"


# pour chaque URL du fichier URL :

lineno=1;

if [[ $lang == 'zh' ]]
then

# nécessaire pour le bon fonctionnement de `sed`
lang_base=$LANG
export LANG=C

while read -r URL;
do
	echo -e "\tURL : $URL";

	# réponse HTTP
	code=$(curl -A "Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:59.0) Gecko/20100101 Firefox/59.0" -ILs $URL | grep -e "^HTTP/" | grep -Eo "[0-9]{3}" | tail -n 1)
	# récupération de l'encodage
	charset=$(curl -A "Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:59.0) Gecko/20100101 Firefox/59.0" -Ls $URL -D - -o "./aspirations/fich-$lineno.html" | grep -Eo "charset=(\w|-)+" | cut -d= -f2)

	if [[ -z $charset ]]
	then
		echo -e "\tencodage non détecté, on prendra UTF-8 par défaut.";
		charset="UTF-8";
	else
		echo -e "\tencodage : $charset";
	fi
	# pour transformer les 'utf-8' en 'UTF-8' :
	charset=$(echo $charset | tr "[a-z]" "[A-Z]")

	# cette page pose problème...
	if [[ $URL == "https://www.cnr.cn/sx/yw/20221129/t20221129_526078646.shtml" ]]
	then
		charset="gb2312"
	fi

	if [[ $code -eq 200 ]]
	then
		aspiration=$(curl -A "Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:59.0) Gecko/20100101 Firefox/59.0" $URL)

		if [[ $charset == 'UTF-8' ]]
		then
			dump=$(curl $URL | iconv -f UTF-8 -t UTF-8//IGNORE | lynx -stdin -dump -assume_charset=utf-8 -display_charset=utf-8 | sed -E "/(BUTTON)/d" | sed -E "/   [*+#_©×]/d" | sed -E "/   \[/d" | sed -E "/^IFRAME/d")
		else
			# charset=$(curl $URL | urchardet)
			dump=$(curl $URL | iconv -f $charset -t UTF-8//IGNORE | lynx -stdin -dump -assume_charset=utf-8 -display_charset=utf-8 | sed -E "/(BUTTON)/d" | sed -E "/   [*+#_©×]/d" | sed -E "/   \[/d" | sed -E "/^IFRAME/d")
		fi
	else
		echo -e "\tcode différent de 200 utilisation d'un dump vide"
		dump=""
		charset=""
	fi

	echo "$aspiration" &gt; "./aspirations/$basename-$lineno.html"

	echo "$dump" &gt; "./dumps-text/$basename-$lineno.txt"
	# segmentation du dump avec thulac, on supprime aussi les indications du scripts qui polluent le dump :
	dumpseg=$(python3 scripts/tokenize_chinese.py "./dumps-text/$basename-$lineno.txt" | sed -E "/Model loaded succeed/d" | sed -E '/References/,$d')
	# on écrase le dump non segmenté
	echo "$dumpseg" &gt; "./dumps-text/$basename-$lineno.txt"

	# compte du nombre d'occurrences
	NB_OCC=$(grep -a -E -o $mot ./dumps-text/$basename-$lineno.txt | wc -l)

	# extraction des contextes
	grep -E -A1 -B1 $mot ./dumps-text/$basename-$lineno.txt &gt; ./contextes/$basename-$lineno.txt

	# construction des concordances avec une commande externe
	bash scripts/concordance.sh zh ./dumps-text/$basename-$lineno.txt $mot &gt; ./concordances/$basename-$lineno.html


	echo "			&lt;tr&gt;&lt;td&gt;$lineno&lt;/td&gt;&lt;td&gt;$code&lt;/td&gt;&lt;td&gt;&lt;a href=\"$URL\"&gt;$URL&lt;/a&gt;&lt;/td&gt;&lt;td&gt;$charset&lt;/td&gt;&lt;td&gt;&lt;a href="../aspirations/$basename-$lineno.html"&gt;html&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a href="../dumps-text/$basename-$lineno.txt"&gt;text&lt;/a&gt;&lt;/td&gt;&lt;td&gt;$NB_OCC&lt;/td&gt;&lt;td&gt;&lt;a href="../contextes/$basename-$lineno.txt"&gt;contextes&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a href="../concordances/$basename-$lineno.html"&gt;concordance&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;" &gt;&gt; "tableaux/$fichier_tableau"
	lineno=$((lineno+1));
done &lt; $fichier_urls

# on revient à la variable globale de base une fois le traitement terminé
export LANG=lang_base

elif [[ $lang == 'ru' ]]
then
while read -r URL;
do
			echo -e "\tURL : $URL";
			code=$(curl -ILs $URL | grep -e "^HTTP/" | grep -Eo "[0-9]{3}" | tail -n 1) # de la manière attendue, sans l'option -w de cURL
			charset=$(curl -ILs $URL | grep -Eo "charset=(\w|-)+" | cut -d= -f2)
			charset=$(echo $charset | tr "[a-z]" "[A-Z]") #ici nous définissons pour la valeur sharset la commande "convertir toutes les petites lettres en majuscules, afin que toutes les lettres UTF-8 soient du même type
			echo -e "\tcode : $code";

			if [[ -z $charset ]]
			then
						echo -e "\tencodage non détecté, on prendra UTF-8 par défaut.";
						charset="UTF-8";
			else
						echo -e "\tencodage : $charset";
			fi

			if [[  $code -eq 200 || $code -eq 403 ]]
						then
						aspiration=$(curl $URL)
						dump=$(lynx -dump -nolist -accept_all_cookies -assume_charset=$charset -display_charset=$charset $URL)
						if [[  $charset -ne 'UTF-8' ]]
						then
							dump=$(iconv -f $charset -t UTF-8)
						fi
			else
						echo -e "\tcode différent de 200 utilisation d'un dump vide"
						dump=""
						charset=""
			fi

			echo "$dump" &gt; "./dumps-text/$basename-$lineno.txt"
			echo "$aspiration" &gt; "./aspirations/$basename-$lineno.html"

# compte du nombre d'occurrences
NB_OCC=$(grep -E -o $mot ./dumps-text/$basename-$lineno.txt | wc -l)
	# extraction des contextes
grep -E -A2 -B2 $mot ./dumps-text/$basename-$lineno.txt &gt; "./contextes/$basename-$lineno.txt" #хз почему но тут и дальше я указал в скобках путь к файлам
	# construction des concordance avec une commande externe
bash scripts/concordance.sh ru ./dumps-text/$basename-$lineno.txt $mot &gt; "./concordances/$basename-$lineno.html"

echo "&lt;tr&gt;&lt;td&gt;$lineno&lt;/td&gt;&lt;td&gt;$code&lt;/td&gt;&lt;td&gt;&lt;a href=\"$URL\"&gt;$URL&lt;/a&gt;&lt;/td&gt;&lt;td&gt;$charset&lt;/td&gt;&lt;td&gt;&lt;a href="../aspirations/$basename-$lineno.html"&gt;html&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a href="../dumps-text/$basename-$lineno.txt"&gt;text&lt;/a&gt;&lt;/td&gt;&lt;td&gt;$NB_OCC&lt;/td&gt;&lt;td&gt;&lt;a href="../contextes/$basename-$lineno.txt"&gt;contextes&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a href="../concordances/$basename-$lineno.html"&gt;concordance&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;" &gt;&gt; "tableaux/$fichier_tableau" #Nous spécifions ici où placer toutes les informations et notons qu'il n'est pas nécessaire de les supprimer à chaque itération
echo -e "\t--------------------------------"

lineno=$((lineno+1));

done  &lt; $fichier_urls

elif [[ $lang == 'fr' ]]
then
while read -r URL;
do
			echo -e "\tURL : $URL";
			code=$(curl -ILs $URL | grep -e "^HTTP/" | grep -Eo "[0-9]{3}" | tail -n 1) # de la manière attendue, sans l'option -w de cURL
			charset=$(curl -ILs $URL | grep -Eo "charset=(\w|-)+" | cut -d= -f2)
			charset=$(echo $charset | tr "[a-z]" "[A-Z]") #ici nous définissons pour la valeur sharset la commande "convertir toutes les petites lettres en majuscules, afin que toutes les lettres UTF-8 soient du même type
			echo -e "\tcode : $code";

			if [[ -z $charset ]]
			then
						echo -e "\tencodage non détecté, on prendra UTF-8 par défaut.";
						charset="UTF-8";
			else
						echo -e "\tencodage : $charset";
			fi

			if [[  $code -eq 200 || $code -eq 403 ]]
				then
						aspiration=$(curl $URL)
						dump=$(lynx -dump -nolist -accept_all_cookies -assume_charset=$charset -display_charset=$charset $URL)
						if [[  $charset -ne 'UTF-8' ]]
						then
							dump=$(iconv -f $charset -t UTF-8)
						fi
			else
						echo -e "\tcode différent de 200 utilisation d'un dump vide"
						dump=""
						charset=""
			fi

			echo "$dump" &gt; "./dumps-text/$basename-$lineno.txt"
			# une fois le fichier créé, on peut utiliser la commande suivante pour supprimer les espaces en début de ligne :
			LANG=C sed -i.bak 's/^[[:space:]]*//' "./dumps-text/$basename-$lineno.txt"
			# une fois les espaces de début de ligne supprimés, on peut nettoyer
			dump_propre=$(cat "./dumps-text/$basename-$lineno.txt" | LANG=C sed '/Également sur RFI/,$d' | LANG=C sed '/(BUTTON)/d' | LANG=C sed '/^[+*©►]/d' | LANG=C sed '/^Price:/,$d' | LANG=C sed '/^http/d' | LANG=C sed '/^IFRAME/d' )
			# et on remplace le dump initial par le dump nettoyé
			echo "$dump_propre" &gt; "./dumps-text/$basename-$lineno.txt"

			echo "$aspiration" &gt; "./aspirations/$basename-$lineno.html"

# compte du nombre d'occurrences
NB_OCC=$(grep -E -o $mot ./dumps-text/$basename-$lineno.txt | wc -l)
	# extraction des contextes
grep -E -A2 -B2 $mot ./dumps-text/$basename-$lineno.txt &gt; "./contextes/$basename-$lineno.txt"
	# construction des concordance avec une commande externe
bash scripts/concordance.sh fr ./dumps-text/$basename-$lineno.txt $mot &gt; "./concordances/$basename-$lineno.html"

echo "&lt;tr&gt;&lt;td&gt;$lineno&lt;/td&gt;&lt;td&gt;$code&lt;/td&gt;&lt;td&gt;&lt;a href=\"$URL\"&gt;$URL&lt;/a&gt;&lt;/td&gt;&lt;td&gt;$charset&lt;/td&gt;&lt;td&gt;&lt;a href="../aspirations/$basename-$lineno.html"&gt;html&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a href="../dumps-text/$basename-$lineno.txt"&gt;text&lt;/a&gt;&lt;/td&gt;&lt;td&gt;$NB_OCC&lt;/td&gt;&lt;td&gt;&lt;a href="../contextes/$basename-$lineno.txt"&gt;contextes&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a href="../concordances/$basename-$lineno.html"&gt;concordance&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;" &gt;&gt; "tableaux/$fichier_tableau" #Nous spécifions ici où placer toutes les informations et notons qu'il n'est pas nécessaire de les supprimer à chaque itération
echo -e "\t--------------------------------"

lineno=$((lineno+1));

done  &lt; $fichier_urls

fi


#on ferme le fichier
echo "		&lt;/table&gt;
	&lt;/body&gt;
&lt;/html&gt;" &gt;&gt; "tableaux/$fichier_tableau"
						</code>
					</pre>
				</div>
				<div class="column is-one-fifth"></div>
			</div>
			<!--concordances-->
			<h1 class="title has-text-centered" style="color: #a92927; font-size: 40px; letter-spacing: -0.02em; font-variant-caps: small-caps;">Concordances</h1>
			<div class="columns">
				<div class="column is-one-fifth"></div>
				<div class="column" style="border-left: 5px solid #a92927; overflow: auto; max-height: 1000px;">
					<p class="block">
						Ce programme nous permet de générer les tableaux de concordances.
					</p>
					<pre>
						<code>
#!/usr/bin/env bash

#
#		Script qui établit une table de concordance en HTML autour d'un motif
#		usage : bash concordance.sh &lt;fichier&gt; &lt;motif&gt;
#
#

lang=$1 # fr, ru, zh
fichier_text=$2
motif=$3

if [[ $# -ne 3 ]]
then
	echo "Ce programme demande exactement trois arguments."
	echo "Usage : $0 &lt;langue&gt; &lt;fichier&gt; &lt;motif&gt;"
	exit
fi

if [[ ! -f $fichier_text ]]
then
	echo "le fichier $fichier_text n'existe pas"
	exit
fi

if [[ -z $motif ]]
then
	echo "le motif est vide"
	exit
fi

if [[ $lang != 'fr' && $lang != 'ru' && $lang != 'zh' ]]
then
	echo "C'est quoi cette langue ?"
	exit
fi

echo 	"
			&lt;html&gt;
				&lt;html lang=\"$lang\"&gt;
				&lt;head&gt;
							&lt;meta charset=\"utf-8\" /&gt;
							&lt;link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css\"&gt;
							&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt;
							&lt;title&gt;Concordance&lt;/title&gt;
			&lt;/head&gt;
			&lt;body class=\"has-navbar-fixed-top\"&gt;
							&lt;nav class=\"navbar is-light is-fixed-top\"&gt;&lt;div class=\"navbar-menu\"&gt;&lt;div class=\"navbar-start\"&gt;&lt;div class=\"navbar-item has-dropdown is-hoverable\"&gt;&lt;a class=\"navbar-item\" href=\"../index.html#introduction\"&gt;Introduction&lt;/a&gt;&lt;/div&gt;&lt;div class=\"navbar-item has-dropdown is-hoverable\"&gt;&lt;a class=\"navbar-item\" href=\"../index.html#analyse\"&gt;Analyse&lt;/a&gt;&lt;/div&gt;&lt;div class=\"navbar-item has-dropdown is-hoverable\"&gt;&lt;a class=\"navbar-item\" href=\"../scripts.html\"&gt;Scripts&lt;/a&gt;&lt;/div&gt;&lt;div class=\"navbar-item has-dropdown is-hoverable\"&gt;&lt;a class=\"navbar-item\"&gt;Tableaux&lt;/a&gt;&lt;div class=\"navbar-dropdown\"&gt;&lt;a class=\"navbar-item\" href=\"../tableaux/tableau_fr.html\"&gt;Français&lt;/a&gt;&lt;a class=\"navbar-item\" href=\"../tableaux/tableau_ru.html\"&gt;Russe&lt;/a&gt;&lt;a class=\"navbar-item\" href=\"../tableaux/tableau_zh.html\"&gt;Chinois&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=\"navbar-end\"&gt;&lt;div class=\"navbar-item has-dropdown is-hoverable\"&gt;&lt;a class=\"navbar-item\" href=\"../index.html#aPropos\"&gt;À propos&lt;/a&gt;&lt;/div&gt;&lt;div class=\"navbar-item\"&gt;&lt;a href=\"https://github.com/cbuontal/M1TAL_immigration\"&gt;&lt;img src=\"../images/github_logo.png\" alt=\"Github\"&gt;&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/nav&gt;
							&lt;h1 class=\"title\"&gt;Concordance&lt;/h1&gt;
							&lt;table class=\"table is-bordered is-striped is-narrow is-hoverable is-fullwidth\"&gt;
									&lt;thead&gt;
									&lt;tr&gt;
									&lt;th class=\"has-text-right\"&gt;Contexte gauche&lt;/th&gt;
									&lt;th&gt;Cible&lt;/th&gt;
									&lt;th class=\"has-text-left\"&gt;Contexte droit&lt;/th&gt;
									&lt;/tr&gt;
									&lt;/thead&gt;
									"

if [[ $lang == 'fr' ]]
then
	grep -E -o -i "(\w+\W+| {0,5})$motif\b(\W+\w+| ){0,5}" $fichier_text | sed -E -r "s/(.*)$motif(.*)/&lt;tr&gt;&lt;td class=\"has-text-right\"&gt;\1&lt;\/td&gt;&lt;td class=\"has-text-danger\"&gt;\2&lt;\/td&gt;&lt;td class=\"has-text-left\"&gt;\3&lt;\/td&gt;&lt;\/tr&gt;/I"
elif [[ $lang == 'ru' ]]
then
	#pour utiliser cette ligne, il faut installer gnu-sed : brew install gnu-sed. sinon utiliser "sed"
	grep -E -o -i "(\w+\W+){0,5}\b$motif\b(\W+\w+){0,5}" $fichier_text | sed -E "s/(.*)$motif(.*)/&lt;tr&gt;&lt;td class=\"has-text-right\"&gt;\1&lt;\/td&gt;&lt;td class=\"has-text-danger\"&gt;\2&lt;\/td&gt;&lt;td class=\"has-text-left\"&gt;\3&lt;\/td&gt;&lt;\/tr&gt;/I"
elif [[ $lang == 'zh' ]]
then
	LANG=zh_CN.UTF-8 grep -Po "((\p{Han}|，){1,5} ){0,5}$motif( (\p{Han}|，){1,5}){0,5}" $fichier_text | LANG=C sed -E "s/(.*)($motif)(.*)/&lt;tr&gt;&lt;td class=\"has-text-right\"&gt;\1&lt;\/td&gt;&lt;td class=\"has-text-danger\"&gt;\2&lt;\/td&gt;&lt;td class=\"has-text-left\"&gt;\3&lt;\/td&gt;&lt;\/tr&gt;/"
fi

echo "
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/body&gt;
&lt;/html&gt;
"
						</code>
					</pre>
				</div>
				<div class="column is-one-fifth"></div>
			</div>
			<!--tokenisation chinois-->
			<h1 class="title has-text-centered" style="color: #a92927; font-size: 40px; letter-spacing: -0.02em; font-variant-caps: small-caps;">Tokenisation du chinois</h1>
			<div class="columns">
				<div class="column is-one-fifth"></div>
				<div class="column" style="border-left: 5px solid #a92927; overflow: auto; max-height: 1000px;">
					<p class="block">
						Ce programme nous permet de tokeniser (mettre des espaces entre les mots) les fichiers en chinois afin de pouvoir les traiter par la suite.
					</p>
					<pre>
						<code>
#!/usr/bin/env python3

import thulac
import errno
import fileinput

# utilisation : python3 tokenize_chinese.py chinois.txt
# possibilité de rediriger la sortie avec >, >>

# autre possibilité, lancer la commande:
# python3 -m thulac chinois.txt chinois_seg_output.txt -seg_only
# Mais ne permet pas les redirections d'entrées/sorties

seg = thulac.thulac(seg_only=True)
try:
	for line in fileinput.input():
		print(seg.cut(line, text=True))
except IOError as e:
	if e.errno != errno.EPIPE:
		raise							
						</code>
					</pre>
				</div>
				<div class="column is-one-fifth"></div>
			</div>
			<!--nuages de mots-->
			<h1 class="title has-text-centered" style="color: #a92927; font-size: 40px; letter-spacing: -0.02em; font-variant-caps: small-caps;">Nuages de mots</h1>
			<div class="columns">
				<div class="column is-one-fifth"></div>
				<div class="column" style="border-left: 5px solid #a92927; overflow: auto; max-height: 1000px;">
					<!--ru fr-->
					<h2 class="title has-text-centered" style="color: #06213e; font-size: 32px; letter-spacing: -0.02em;">Russe et français</h2>
					<p class="block">
						Ce programme nous permet de générer des nuages de mots pour le russe et le français. Ici, l'exemple est donné pour le russe. Un autre fichier est disponible adapté au français.
					</p>
					<pre>
						<code>
import string
import re
import matplotlib.pyplot as plt
import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize
from wordcloud import WordCloud

## À EXÉCUTER DEPUIS LA RACINE DU DOSSIER

text = str()
with open("./dumps-text/ru_total2.txt", "r") as f:
	text = f.read()

# passage en minuscule et suppression des ponctuations et lignes vides
text = text.lower()
spec_chars = string.punctuation + '\n\xa0«»\t—…'
text = re.sub('\n', '', text)
text = "".join([ch for ch in text if ch not in spec_chars])
# suppression des mots contenant des caractères latins
mots_texte = text.split()
for idx, mot in enumerate(mots_texte):
	if 'a' in mot or 'b' in mot or 'c' in mot or 'd' in mot or 'e' in mot or 'f' in mot or 'g' in mot or 'h' in mot or 'i' in mot or 'j' in mot or 'k' in mot or 'l' in mot or 'm' in mot or 'n' in mot or 'o' in mot or 'p' in mot or 'q' in mot or 'r' in mot or 's' in mot or 't' in mot or 'u' in mot or 'v' in mot or 'w' in mot or 'x' in mot or 'y' in mot or 'z' in mot :
		mots_texte.remove(mot)
text = " ".join(mots_texte)

text_tokens = word_tokenize(text)
text = nltk.Text(text_tokens)

russian_stopwords = stopwords.words("russian")
#liste de mots indésirables
russian_stopwords.extend(['это', 'iframe', 'году', 'нею', 'button', 'twitter', 'telegram', 'livejournal', 'года', 'который', 'изза'])

text_tokens = [token.strip() for token in text_tokens if token not in russian_stopwords]
text = nltk.Text(text_tokens)

## création du nuage de mots
text_raw = " ".join(text)
wordcloud = WordCloud(max_font_size=50, max_words=100, background_color="white").generate(text_raw)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()
						</code>
					</pre>
					<!--zh-->
					<h2 class="title has-text-centered" style="color: #06213e; font-size: 32px; letter-spacing: -0.02em;">Chinois</h2>
					<p class="block">
						Ce programme nous permet de générer des nuages de mots pour le chinois.
					</p>
					<pre>
						<code>
import jieba, codecs, sys, pandas
import numpy as np
from wordcloud import WordCloud
from imageio import imread
from wordcloud import WordCloud, ImageColorGenerator
from os import listdir
from os.path import isfile, join
import matplotlib.pyplot as plt

#
#   Pour bien fonctionner, ce script a besoin de deux fichiers :
#       - un fichier .txt contenant les stopwords chinois
#       - un fichier .ttf contenant la police à utiliser
#
#   Ces fichiers sont rangés dans un sous-dossier data/
#
#



stopwords_filename = 'data/stopwords_zh.txt'
font_filename = 'data/simkai.ttf'

def contain_latin(txt):
	return ('a' in txt or 'b' in txt or 'c' in txt or 'd' in txt or 'e' in txt or 'f' in txt or 'g' in txt or 'h' in txt or 'i' in txt or 'j' in txt or 'k' in txt or 'l' in txt or 'm' in txt or 'n' in txt or 'o' in txt or 'p' in txt or 'q' in txt or 'r' in txt or 's' in txt or 't' in txt or 'u' in txt or 'v' in txt or 'w' in txt or 'x' in txt or 'y' in txt or 'z' in txt)

def filter_numbers(txt):
	"""
		Renvoie vrai si l'argument passé en paramètre est une chaîne de caractères
		constituée uniquement de chiffres et de longueur strictement supérieure à 4
		- renvoie False si la chaîne est une suite de chiffres de longueur &gt; 4
		- True sinon

		But = ne garder que les nombres assimilables à des années, ou éventuellement
		des pourcentages
	"""
	if txt.isdigit() and len(txt) &gt; 4:
		return False
	else:
		return True

def main(input_filename):
	content = '\n'.join([line.strip()
						for line in codecs.open(input_filename, 'r', 'utf-8')
						if len(line.strip()) &gt; 0])
	stopwords = set([line.strip()
					for line in codecs.open(stopwords_filename, 'r', 'utf-8')])

	segs = jieba.cut(content)
	words = []
	for seg in segs:
		word = seg.strip().lower()
		# nettoyage des mots indésirables
		if len(word) &gt; 1 and word not in stopwords and not contain_latin(word) and filter_numbers(word) and word != '-%':
			words.append(word)

	words_df = pandas.DataFrame({'word':words})
	words_stat = words_df.groupby(by=['word'])['word'].agg(number=np.size)
	words_stat = words_stat.reset_index().sort_values(by="number",ascending=False)

	print('# of different words =', len(words_stat))

	wordcloud = WordCloud(font_path=font_filename, max_font_size=50, random_state=100, max_words=100, background_color="white")
	wordcloud = wordcloud.fit_words(dict(words_stat.head(4000).itertuples(index=False)))
	print("Sauvegarde du nuage")
	wordcloud.to_file('nuage_chinois.png')

if __name__ == '__main__':
	if len(sys.argv) == 2:
		main(sys.argv[1])
	else:
		print('[usage] &lt;input&gt;')														
						</code>
					</pre>
				</div>
				<div class="column is-one-fifth"></div>
			</div>
    	</div>
	</body>
</html>